\section{Control Net Reduction} \label{sec:cnr}
Control net reduction, CNR, is the natural extension from uni-variable B-splines
to multi-variable B-splines.   This section is a brief overview of the extension
and tools in the \pkg{cpr} package for use in this case.

\subsection{Multi-variable B-Splines}
We generalize \eqref{eq:cpr_general_model} to have a multi-variable function as
the varying mean element, \ie,
\begin{equation}
  \label{eq:cnr_general_model}
  \bs{y} = f \left( \bs{x}_1, \bs{x}_2, \ldots, \bs{x}_m \right) + \bs{Z}_{f} \bs{\beta} + \bs{Z}_{r} \bs{b} +
  \bs{\epsilon}.
\end{equation}

Multi-variable B-spline functions are constructed by tensor products of
uni-variable B-splines bases, henceforth referred to as marginal B-splines.  We
define the multi-variable B-spline function in terms of matrix arithmetic and as
an algebraic formula, the latter will be useful in assessing the influence of a
knot on a tensor product.

We denote a multi-variable $m$-dimensional B-spline function, built on $m$ B-spline bases
$\bs{B}_{k_1, \bs{\xi}_1} \left(\bs{x}_1\right),$
$\bs{B}_{k_2, \bs{\xi}_2} \left(\bs{x}_2\right),$ \ldots,
$\bs{B}_{k_m, \bs{\xi}_m} \left(\bs{x}_m\right),$ as
\begin{equation}
  \label{eq:bspline_multivariable_function}
  f\left( \bs{X} \right) = \mathscr{B}_{\bs{K}, \bs{\Xi}} \left(\bs{X}\right)
  \bs{\theta}_{\bs{\Xi}},
\end{equation}
where 
$\bs{K} = \left\{k_1, k_2, \ldots, k_m \right\}$, denotes the set of polynomial
orders,
$\bs{\Xi} = \left\{\bs{\xi}_1, \bs{\xi}_2, \ldots, \bs{\xi}_m \right\},$ is the
set of knot sequences, 
$\bs{\theta}_{\bs{\Xi}}$ is a $\prod_{i=1}^{m}
\left(\card{\bs{\xi}_i} - k_i \right) \times 1$ column vector of
regression coefficients,  and
$\bs{X}$ is the observed data
\begin{equation}
  \bs{X} = \begin{pmatrix}
    x_{11} & x_{21} & \cdots & x_{m1} \\ 
    x_{12} & x_{22} & \cdots & x_{m2} \\ 
    \vdots & \vdots & \ddots & \vdots \\
    x_{12} & x_{22} & \cdots & x_{mn}
  \end{pmatrix}.
\end{equation}

The basis for multi-variable B-splines is constructed by a recursive algorithm.
The base case for $m = 2$ is
\begin{equation}
  \label{eq:tensor_product_base_case}
  \mathscr{B}_{ \left\{k_1, k_2\right\}, \left\{ \bs{\xi}_1, \bs{\xi}_2 \right\}} \left( \bs{x}_1, \bs{x}_2 \right) = 
  \left(\bs{1}^T_{\card{\bs{\xi}_2} - k_2} \otimes \bs{B}_{k_1, \bs{\xi}_1} \left(\bs{x}_1 \right)  \right) \odot
  \left(\bs{B}_{k_2, \bs{\xi}_2} \left( \bs{x}_2 \right) \otimes
  \bs{1}^{T}_{\card{\bs{\xi}_1} - k_1} \right),
\end{equation} 
where $\odot$ is the element-wise product, $\otimes$ is a
Kronecker product, and $\bs{1}_n$ is a $n \times 1$ column vector of 1s.  The
two Kronecker products define the correct dimensions for the entry-wise product.
The tensor product matrix as the same number of rows as the two input matrices
and the columns are generated by all the pairwise products of the columns of the
two input matrices.
The general case for $m > 2,$ the matrix $\mathscr{B}_{\bs{K}, \bs{\Xi}} \left( \bs{X} \right)$ is defined by
\begin{equation}
  \label{eq:tensor_product_recursive}
    \mathscr{B}_{\bs{K}, \bs{\Xi}} \left( \bs{X} \right) =
    \left( \bs{1}_{\card{\bs{\xi}_m} - k_m}^{T} \otimes \mathscr{B}_{\bs{K} \backslash
  k_{m}, \bs{\Xi} \backslash \bs{\xi}_m} \left(\bs{X} \backslash \bs{x}_{m} \right) \right)
    \odot
    \left( \bs{B}_{k_m, \bs{\xi}_m}\left(\bs{x}_m\right) \otimes
    \bs{1}_{\prod_{i = 1}^{m-1} \left(\card{\bs{\xi}_i} - k_i \right)}^{T}
  \right).
\end{equation}

It is possible to write \eqref{eq:bspline_multivariable_function} as a set of
summations as follows:
\begin{equation}
  \label{eq:bspline_multivariable_function_sum}
  \begin{split}
    f\left( \bs{X} \right) & = \mathscr{B}_{\bs{K}, \bs{\Xi}} \left( \bs{X} \right) \\
   & =
    \sum_{j_1 = 1}^{\card{\bs{\xi}_1} - k_1}
    \sum_{j_2 = 1}^{\card{\bs{\xi}_2} - k_2}
  \cdots
  \sum_{j_m = 1}^{\card{\bs{\xi}_m} - k_m}
  \bs{B}_{j_1, k_1, \bs{\xi}_1} \left( \bs{x}_1 \right)
  \bs{B}_{j_2, k_2, \bs{\xi}_2} \left( \bs{x}_2 \right)
  \cdots
  \bs{B}_{j_m, k_m, \bs{\xi}_m} \left( \bs{x}_m \right)
  \theta_{\bs{\Xi}, j_1, j_2, \ldots, j_m} \\
  & =
    \sum_{j_1 = 1}^{\card{\bs{\xi}_1} - k_1} \bs{B}_{j_1, k_1, \bs{\xi}_1} \left( \bs{x}_1 \right) 
  \underbrace{
    \sum_{j_2 = 1}^{\card{\bs{\xi}_2} - k_2}
  \cdots
  \sum_{j_m = 1}^{\card{\bs{\xi}_m} - k_m}
  \bs{B}_{j_2, k_2, \bs{\xi}_2} \left( \bs{x}_2 \right)
  \cdots
  \bs{B}_{j_m, k_m, \bs{\xi}_m} \left( \bs{x}_m \right)
  \theta_{\bs{\Xi}, j_1, j_2, \ldots, j_m}
  }_{\text{polynomial coefficients}} \\
  & = 
  \text{diag}\left(
    \bs{B}_{k_1, \bs{\xi}_1}\left(\bs{x}_1\right)\bs{\theta}_{\bs{\Xi}\backslash \bs{\xi}_1} \left( \bs{X} \backslash \bs{x}_1
    \right)
  \right).
  \end{split}
\end{equation}
When the data input is a single observation, that is a tuple $\left(x_1, x_2,
\ldots, x_m\right) \in \text{rows}\left(\bs{X}\right)$ the last line is a
singleton and the diag operation is redundant.  Equation
\eqref{eq:bspline_multivariable_function_sum} is critical in the extension from
the uni-variable control polygon reduction method to the multi-variable control
polygon reduction method.  By conditioning on $m-1$ marginals, the
multi-variable B-spline becomes a uni-variable B-spline in terms of the $m^{th}$
marginal.  Thus, the metrics and methods developed
for uni-variable B-splines can be applied to multi-variable B-splines.



To simplify the explanation consider a $m = 2$ dimensional B-spline.  The
concepts and notation extend to $m > 2$ with ease.
If we condition on $x_2,$ the two-dimensional B-spline simplifies to a
uni-variable spline in $x_1,$ \ie,
\begin{equation}
  \label{eq:conditional_uni_var_bspline}
  \mathscr{B}_{\bs{K}, \bs{\Xi}} \left(x_1 | x_2 \right) \bs{\theta}_{\bs{\Xi}} = 
  \bs{B}_{k_1, \bs{\xi}_1}\left(x_1\right)\bs{\theta}_{\bs{\Xi}\backslash \bs{\xi}_1}
    \left( x_2 \right).
\end{equation}
In terms of the control net, the conditioning on $x_2$ creates a slice across the
net and yields the control polygon $\CP_{k_1, \bs{\xi}_1, \bs{\theta}_{\bs{\Xi} \backslash \bs{\xi}_1}\left(x_2\right)}.$


The influence weight of $\xi_{1j} \in \bs{\xi}_1$
is the Euclidean distance between the ordinates $\bs{\theta}_{\bs{\Xi} \backslash \bs{\xi}_1}\left(x_2\right)$
and $\bs{\theta}_{\bs{\Xi} \backslash \left( \left( \bs{\xi}_1 \backslash \xi_{1j} \right) \cup \xi_{1j} \right)}\left(x_2\right).$
\begin{equation}
  \label{eq:cnr_conditional_influence_weights}
  w_{1j | x_2} =
  \left\lVert
    \left(
      \bs{I} - 
      \bs{W}_{k_1, \bs{\xi}_1}\left(\xi_{1j}\right)
      \left( \bs{W}_{k_1, \bs{\xi}_1}^{T}\left(\xi_{1j}\right) \bs{W}_{k_1, \bs{\xi}_1}\left(\xi_{1j}\right) \right)^{-1} \bs{W}_{k_1, \bs{\xi}_1}^{T}\left(\xi_{1j}\right)
    \right)
    \bs{\theta}_{\bs{\Xi} \backslash \bs{\xi}_1} \left( x_2 \right)
  \right\rVert_2.
\end{equation}

The conditional influence weight, \eqref{eq:cnr_conditional_influence_weights},
is used to get the influence weight of $\xi_{1j}$ on
$\CP_{k_1, \bs{\xi}_1, \bs{\theta}_{\bs{\Xi} \backslash
\bs{\xi}_1}\left(x_2\right)}.$  That is, the relative influence weight of
$\xi_{1j}$ is the maximum influence weight over a set of $p$ values for $x_2.$
\begin{equation}
  \label{eq:cnr_influence_weight}
  w_{1j} = \max_{x_2 \in \bs{U}} w_{1j | x_2},
\end{equation}
where
\begin{equation}
  \bs{U} = \left\{u : \min \left( x_2 \right) + \frac{ \left\{1, 2, \ldots, p \right\} }{p+1} \left( \max\left( x_2  \right) - \min\left(x_2\right) \right) \right\}.
\end{equation}
We recommend and have set the default in \pkg{cpr} to use $p = 20$ values for each marginal.


\subsection{The CNR Algorithm}
\begin{enumerate}
  \item Define the initial $\bs{K}$ and $\bs{\Xi}$ set for
    the initial tensor product.
  \item Use an appropriate regression modeling approach to fit a regression
    model for~\eqref{eq:cnr_general_model}. 
  \item \label{cnr_step:build} Construct the control net for the current
    set of knots sequences and regression coefficients.
  \item Use \eqref{eq:cnr_influence_weight} to find the influence weight
    for all internal knots on the marginals you are interested in reducing.
  \item Coarsen the knot sequence by removing the knot with
    the smallest influence weight.
  \item\label{cnr_step:refit} Refit the regression model using the coarsened knot
    sequences.
  \item Repeat steps \ref{cnr_step:build} through \ref{cnr_step:refit} until all
    internal knots have been removed.
  \item\label{cnr_step:selection} Select the preferable model by visual inspection
    of diagnostic graphics.
\end{enumerate}

CNR is applied to all margins of interest at once.  The knot with the lowest
influence weight is omitted at each step, regardless of the margin it comes
from.  The qualification ``margin of interest'' is explained in the following
section on implementation issues.  It is possible, and reasonable, in some cases
to build a control net and only want to apply CNR to a subset of the margins.

\subsection{Use In The cpr Package}
The use of the CNR algorithm in the \pkg{cpr} package is very similar to the use
of the CPR algorithm.  Use \code{cpr::btensor} to generate the multi-variable
B-spline, \code{cn} to generate the control net, \code{cnr} to apply the CNR
algorithm, and \code{plot} to see diagnostic plots.  A simple example, fitting a
two-dimensional B-spline over \code{day} and \code{age}, with two fourth order
splines is below.  The example also shows an updated version of the initial
control net with a third order B-spline for \code{day} and fourth order spline
for \code{age}.
<<eval = FALSE>>=
initial_cn44 <-
  cn(log10(pdg) ~ btensor(list(day, age), df = list(24, 24)) + (1 | id),
     data = spdg, method = lmer) 
initial_cn34 <- update_btensor(df = list(23, 24), order = list(3, 4))
cnr_run44 <- cnr(initial_cn44)
cnr_run34 <- cnr(initial_cn34)
plot(cnr_run44)
@


